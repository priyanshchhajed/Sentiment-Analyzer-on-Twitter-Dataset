{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108804df-8f18-4a8c-87c8-53138b9dc1a5",
   "metadata": {},
   "source": [
    "***Model Traning Code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bee428-903b-44f2-878e-a449708d24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @crnixon i'd be interested in speaking, but no...   \n",
      "1  @rachelhart @rachelhart You really like changi...   \n",
      "2  Off to Ciara's swimming lesson later, looking ...   \n",
      "3   pain errrg! stupid cold and stupid people con...   \n",
      "4  @ddlovato CONGRATULATIONS DEMI  . YOU'RE THE B...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  id be interested in speaking but no idea what ...   \n",
      "1  you really like changing you profile pic dont you   \n",
      "2  off to ciaras swimming lesson later looking fo...   \n",
      "3  pain errrg stupid cold and stupid people confu...   \n",
      "4  congratulations demi youre the best i love you lt   \n",
      "\n",
      "                                   no_stopwords_text  polarity  \n",
      "0  [id, interested, speaking, no, idea, id, talk,...    0.3182  \n",
      "1       [really, like, changing, profile, pic, dont]    0.4201  \n",
      "2  [ciaras, swimming, lesson, later, looking, for...    0.0000  \n",
      "3  [pain, errrg, stupid, cold, stupid, people, co...   -0.9001  \n",
      "4     [congratulations, demi, youre, best, love, lt]    0.9231  \n",
      "TF-IDF Vectorization Time: 1.1609 seconds\n",
      "Test Data - Precision: 0.7645\n",
      "Test Data - Recall: 0.7643\n",
      "Test Data - F1 Score: 0.7643\n",
      "Accuracy score of the test data: 0.7643333333333333\n",
      "Training Data - Precision: 0.8451\n",
      "Training Data - Recall: 0.8450\n",
      "Training Data - F1 Score: 0.8450\n",
      "Accuracy score of the training data: 0.845\n",
      "Elapsed time: 1877.72Â seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import nltk\n",
    "import string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # Changed from TextBlob to VADER\n",
    "\n",
    "# # Load NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')  # Download VADER lexicon\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load data\n",
    "twitter_data = pd.read_csv('dataset_1 (1).csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Cleaning function\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'<.*?>', '', tweet)  # Remove HTML tags\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)  # Remove hashtags\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    tweet = re.sub(r'\\d+', '', tweet)  # Remove digits\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()  # Remove extra spaces\n",
    "    return tweet\n",
    "\n",
    "# Apply cleaning\n",
    "twitter_data['cleaned_text'] = twitter_data['text'].apply(clean_tweet)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_tweet(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "\n",
    "twitter_data['tokenized_text'] = twitter_data['cleaned_text'].apply(tokenize_tweet)\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"not\", \"no\", \"never\", \"n't\"}  # Keep negation words\n",
    "\n",
    "# Remove stop words but keep negation words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words or word in negation_words]\n",
    "\n",
    "twitter_data['no_stopwords_text'] = twitter_data['tokenized_text'].apply(remove_stopwords)\n",
    "\n",
    "# *New Sentiment Analysis Using VADER*\n",
    "def get_sentiment(tweet):\n",
    "    sentiment_score = sia.polarity_scores(tweet)  # Get sentiment scores\n",
    "    return sentiment_score['compound']  # Return overall polarity (-1 to +1)\n",
    "\n",
    "# Apply VADER Sentiment Analysis\n",
    "twitter_data['polarity'] = twitter_data['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Print processed results\n",
    "print(twitter_data[['text', 'cleaned_text', 'no_stopwords_text', 'polarity']].head())\n",
    "\n",
    "# Stemming function\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(content):\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stop_words]\n",
    "    return ' '.join(stemmed_content)\n",
    "\n",
    "# Batch processing for stemming\n",
    "def batch_process_stemming(data, batch_size=10000):\n",
    "    stemmed_contents = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        stemmed_batch = batch.apply(stemming)\n",
    "        stemmed_contents.extend(stemmed_batch)\n",
    "    return stemmed_contents\n",
    "\n",
    "# Apply stemming\n",
    "twitter_data['stemmed_content'] = batch_process_stemming(twitter_data['no_stopwords_text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Prepare feature and target variables\n",
    "X = twitter_data['stemmed_content'].values\n",
    "Y = twitter_data['target'].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=2)\n",
    "\n",
    "# Check if 'polarity' column exists\n",
    "if 'polarity' in twitter_data.columns:\n",
    "    numeric_features = twitter_data[['polarity']] # Replace or add more features if applicable\n",
    "    \n",
    "    # Split numeric features based on train-test split\n",
    "    numeric_train, numeric_test = train_test_split(numeric_features, test_size=0.3, stratify=Y, random_state=2)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    numeric_train_array = numeric_train.values\n",
    "    numeric_test_array = numeric_test.values\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    zscore_train = scaler.fit_transform(numeric_train_array)\n",
    "    zscore_test = scaler.transform(numeric_test_array)\n",
    "\n",
    "    # Convert to sparse matrices\n",
    "    zscore_train_sparse = csr_matrix(zscore_train)\n",
    "    zscore_test_sparse = csr_matrix(zscore_test)\n",
    "\n",
    "    # Time for TF-IDF vectorization\n",
    "    start_time = time.time()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    vectorization_time = time.time() - start_time\n",
    "    print(f\"TF-IDF Vectorization Time: {vectorization_time:.4f} seconds\")\n",
    "\n",
    "    # Combine features\n",
    "    combined_train_sparse = hstack([X_train_tfidf, zscore_train_sparse])\n",
    "    combined_test_sparse = hstack([X_test_tfidf, zscore_test_sparse])\n",
    "\n",
    "    # Initialize and fit the SVM model\n",
    "    svm_model = SVC(kernel='linear')\n",
    "    svm_model.fit(combined_train_sparse, Y_train)\n",
    "\n",
    "    # Predictions for test data\n",
    "    X_test_prediction = svm_model.predict(combined_test_sparse)\n",
    "\n",
    "    # Evaluation metrics for test data\n",
    "    test_precision = precision_score(Y_test, X_test_prediction, average='weighted')\n",
    "    test_recall = recall_score(Y_test, X_test_prediction, average='weighted')\n",
    "    test_f1 = f1_score(Y_test, X_test_prediction, average='weighted')\n",
    "\n",
    "    print(f'Test Data - Precision: {test_precision:.4f}')\n",
    "    print(f'Test Data - Recall: {test_recall:.4f}')\n",
    "    print(f'Test Data - F1 Score: {test_f1:.4f}')\n",
    "\n",
    "    # Accuracy score on testing data\n",
    "    test_data_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
    "    print('Accuracy score of the test data:', test_data_accuracy)\n",
    "\n",
    "    # Predictions for training data\n",
    "    X_train_prediction = svm_model.predict(combined_train_sparse)     # with blancing factor\n",
    "\n",
    "    # Evaluation metrics for training data\n",
    "    train_precision = precision_score(Y_train, X_train_prediction, average='weighted')\n",
    "    train_recall = recall_score(Y_train, X_train_prediction, average='weighted')\n",
    "    train_f1 = f1_score(Y_train, X_train_prediction, average='weighted')\n",
    "\n",
    "    print(f'Training Data - Precision: {train_precision:.4f}')\n",
    "    print(f'Training Data - Recall: {train_recall:.4f}')\n",
    "    print(f'Training Data - F1 Score: {train_f1:.4f}')\n",
    "\n",
    "    # Accuracy score on training data\n",
    "    training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "    print('Accuracy score of the training data:', training_data_accuracy)\n",
    "else:\n",
    "    print(\"Error: 'polarity' column not found in the dataset.\")\n",
    "\n",
    "# End measuring time and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f}Â seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec65f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization Time: 1.1503 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\Priyansh\\HCI\\hcien\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:15:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Data - Precision: 0.7550\n",
      "XGBoost Test Data - Recall: 0.7515\n",
      "XGBoost Test Data - F1 Score: 0.7505\n",
      "XGBoost Test Data - Accuracy: 0.7515\n",
      "XGBoost Train Data - Precision: 0.7723\n",
      "XGBoost Train Data - Recall: 0.7687\n",
      "XGBoost Train Data - F1 Score: 0.7678\n",
      "XGBoost Train Data - Accuracy: 0.7687\n",
      "Elapsed time: 32.27 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import string\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load data\n",
    "twitter_data = pd.read_csv('dataset_1 (1).csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Cleaning function\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'<.*?>', '', tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Apply cleaning\n",
    "twitter_data['cleaned_text'] = twitter_data['text'].apply(clean_tweet)\n",
    "\n",
    "# Tokenization\n",
    "twitter_data['tokenized_text'] = twitter_data['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "\n",
    "# Remove stopwords (keep negation words)\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words or word in negation_words]\n",
    "\n",
    "twitter_data['no_stopwords_text'] = twitter_data['tokenized_text'].apply(remove_stopwords)\n",
    "\n",
    "# Sentiment analysis using VADER\n",
    "def get_sentiment(tweet):\n",
    "    sentiment_score = sia.polarity_scores(tweet)\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "twitter_data['polarity'] = twitter_data['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Stemming\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(content):\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stop_words]\n",
    "    return ' '.join(stemmed_content)\n",
    "\n",
    "# Batch stemming\n",
    "def batch_process_stemming(data, batch_size=10000):\n",
    "    stemmed_contents = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        stemmed_batch = batch.apply(stemming)\n",
    "        stemmed_contents.extend(stemmed_batch)\n",
    "    return stemmed_contents\n",
    "\n",
    "# Apply stemming\n",
    "twitter_data['stemmed_content'] = batch_process_stemming(twitter_data['no_stopwords_text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Prepare features and target\n",
    "X = twitter_data['stemmed_content'].values\n",
    "Y = twitter_data['target'].values\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=2)\n",
    "\n",
    "# Feature: polarity\n",
    "numeric_features = twitter_data[['polarity']]\n",
    "numeric_train, numeric_test = train_test_split(numeric_features, test_size=0.3, stratify=Y, random_state=2)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "zscore_train = scaler.fit_transform(numeric_train.values)\n",
    "zscore_test = scaler.transform(numeric_test.values)\n",
    "\n",
    "# Convert to sparse matrix\n",
    "zscore_train_sparse = csr_matrix(zscore_train)\n",
    "zscore_test_sparse = csr_matrix(zscore_test)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "start_time = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "vectorization_time = time.time() - start_time\n",
    "print(f\"TF-IDF Vectorization Time: {vectorization_time:.4f} seconds\")\n",
    "\n",
    "# Combine TF-IDF + polarity\n",
    "combined_train_sparse = hstack([X_train_tfidf, zscore_train_sparse])\n",
    "combined_test_sparse = hstack([X_test_tfidf, zscore_test_sparse])\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model.fit(combined_train_sparse, Y_train)\n",
    "\n",
    "# Predictions - Test\n",
    "X_test_prediction = xgb_model.predict(combined_test_sparse)\n",
    "test_precision = precision_score(Y_test, X_test_prediction, average='weighted')\n",
    "test_recall = recall_score(Y_test, X_test_prediction, average='weighted')\n",
    "test_f1 = f1_score(Y_test, X_test_prediction, average='weighted')\n",
    "test_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
    "\n",
    "print(f'XGBoost Test Data - Precision: {test_precision:.4f}')\n",
    "print(f'XGBoost Test Data - Recall: {test_recall:.4f}')\n",
    "print(f'XGBoost Test Data - F1 Score: {test_f1:.4f}')\n",
    "print(f'XGBoost Test Data - Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predictions - Train\n",
    "X_train_prediction = xgb_model.predict(combined_train_sparse)\n",
    "train_precision = precision_score(Y_train, X_train_prediction, average='weighted')\n",
    "train_recall = recall_score(Y_train, X_train_prediction, average='weighted')\n",
    "train_f1 = f1_score(Y_train, X_train_prediction, average='weighted')\n",
    "train_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "\n",
    "print(f'XGBoost Train Data - Precision: {train_precision:.4f}')\n",
    "print(f'XGBoost Train Data - Recall: {train_recall:.4f}')\n",
    "print(f'XGBoost Train Data - F1 Score: {train_f1:.4f}')\n",
    "print(f'XGBoost Train Data - Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Total elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac3c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'C': 0.1, 'class_weight': 'balanced'}\n",
      "\n",
      "--- Test Data Metrics ---\n",
      "Accuracy: 0.7811666666666667\n",
      "Precision: 0.7816842332443094\n",
      "Recall: 0.7811666666666667\n",
      "F1 Score: 0.7810455675157685\n",
      "\n",
      "--- Train Data Metrics ---\n",
      "Accuracy: 0.8156571428571429\n",
      "Precision: 0.8160192817760887\n",
      "Recall: 0.8156571428571429\n",
      "F1 Score: 0.8155917887643632\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load data\n",
    "twitter_data = pd.read_csv('dataset_1 (1).csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Cleaning function\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'<.*?>', '', tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "twitter_data['cleaned_text'] = twitter_data['text'].apply(clean_tweet)\n",
    "\n",
    "# Tokenization and Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "\n",
    "def preprocess_tokens(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return [word for word in tokens if word not in stop_words or word in negation_words]\n",
    "\n",
    "twitter_data['tokens'] = twitter_data['cleaned_text'].apply(preprocess_tokens)\n",
    "\n",
    "# Stemming\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(tokens):\n",
    "    return ' '.join([port_stem.stem(word) for word in tokens])\n",
    "\n",
    "twitter_data['stemmed_content'] = twitter_data['tokens'].apply(stemming)\n",
    "\n",
    "# VADER polarity\n",
    "twitter_data['polarity'] = twitter_data['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Additional Features\n",
    "twitter_data['text_length'] = twitter_data['text'].apply(lambda x: len(x))\n",
    "twitter_data['exclamations'] = twitter_data['text'].apply(lambda x: x.count('!'))\n",
    "twitter_data['all_caps'] = twitter_data['text'].apply(lambda x: sum(1 for w in x.split() if w.isupper()))\n",
    "\n",
    "# Features and Labels\n",
    "X_text = twitter_data['stemmed_content']\n",
    "Y = twitter_data['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train_text, X_test_text, y_train, y_test, train_df, test_df = train_test_split(\n",
    "    X_text, Y, twitter_data, test_size=0.3, stratify=Y, random_state=2\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorizer with unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Numerical Features\n",
    "num_features = ['polarity', 'text_length', 'exclamations', 'all_caps']\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(train_df[num_features])\n",
    "X_test_num = scaler.transform(test_df[num_features])\n",
    "\n",
    "X_train_combined = hstack([X_train_tfidf, csr_matrix(X_train_num)])\n",
    "X_test_combined = hstack([X_test_tfidf, csr_matrix(X_test_num)])\n",
    "\n",
    "# Train SVM with hyperparameter tuning\n",
    "params = {'C': [0.01, 0.1, 1, 10], 'class_weight': [None, 'balanced']}\n",
    "grid = GridSearchCV(LinearSVC(), params, scoring='f1_weighted', cv=5)\n",
    "grid.fit(X_train_combined, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test_combined)\n",
    "y_pred_train = best_model.predict(X_train_combined)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Best SVM Parameters:\", grid.best_params_)\n",
    "print(\"\\n--- Test Data Metrics ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='weighted'))\n",
    "\n",
    "print(\"\\n--- Train Data Metrics ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Precision:\", precision_score(y_train, y_pred_train, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_train, y_pred_train, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_pred_train, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca105ca-76ce-40ab-9e81-e113eb1c3f64",
   "metadata": {},
   "source": [
    "***Model Saving in .pkl format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b26b4f-2f36-4669-948e-d927d47a421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained SVM model\n",
    "joblib.dump(svm_model, 'sentiment_svm_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the StandardScaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"Model, vectorizer, and scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8730c88-35b2-49bb-b6b8-dabe9997ea6e",
   "metadata": {},
   "source": [
    "***Using the SVM model in the Tool    (Part 1 )***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3826c9-9d8b-43cf-b671-0710f9820d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageTk\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load the saved SVM model, vectorizer, and scaler\n",
    "svm_model = joblib.load(\"sentiment_svm_model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment of a single text string.\"\"\"\n",
    "    sentence_polarity = sia.polarity_scores(text)[\"compound\"]\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    polarity_array = np.array([[sentence_polarity]])\n",
    "    scaled_polarity = scaler.transform(polarity_array)\n",
    "    scaled_polarity_sparse = csr_matrix(scaled_polarity)\n",
    "    combined_features = hstack([text_tfidf, scaled_polarity_sparse])\n",
    "\n",
    "    if combined_features.shape[1] != svm_model.n_features_in_:\n",
    "        return None, None, f\"Feature mismatch: Model expects {svm_model.n_features_in_} features, but got {combined_features.shape[1]}.\"\n",
    "\n",
    "    prediction = svm_model.predict(combined_features)[0]\n",
    "\n",
    "    if sentence_polarity > 0.1:\n",
    "        sentiment = \"Positive\"\n",
    "    elif -0.1 <= sentence_polarity <= 0.1:\n",
    "        sentiment = \"Neutral\"\n",
    "    else:\n",
    "        sentiment = \"Negative\"\n",
    "\n",
    "    return sentiment, sentence_polarity, None  # No error\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "categories = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"AI\", \"data\", \"computer\", \"cloud\", \"robotics\", \"internet\"],\n",
    "    \"Entertainment\": [\"movie\", \"music\", \"show\", \"film\", \"concert\", \"actor\", \"director\"],\n",
    "    \"Politics\": [\"election\", \"government\", \"president\", \"policy\", \"vote\", \"law\", \"senate\"],\n",
    "    \"Sports\": [\"football\", \"basketball\", \"cricket\", \"match\", \"goal\", \"tennis\", \"athlete\"],\n",
    "    \"General Fun\": [\"fun\", \"joke\", \"meme\", \"party\", \"game\", \"laugh\", \"entertainment\"],\n",
    "    \"News\": [\"breaking\", \"news\", \"report\", \"update\", \"headline\", \"journalist\"]\n",
    "}\n",
    "\n",
    "def classify_tweet(text):\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in text for word in keywords):\n",
    "            return category\n",
    "    return \"Other\"\n",
    "\n",
    "def analyze_dataset_thread(filepath):\n",
    "    \"\"\"Analyzes sentiment and classifies tweets of a dataset in a separate thread and updates the UI in the same window.\"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath)\n",
    "        elif filepath.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(filepath)\n",
    "        elif filepath.endswith('.txt'):\n",
    "            df = pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "        text_column = None\n",
    "        for col in df.columns:\n",
    "            if \"text\" in col.lower() or \"review\" in col.lower() or \"comment\" in col.lower() or \"content\" in col.lower() or \"message\" in col.lower() or \"description\" in col.lower():\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if text_column is None:\n",
    "            messagebox.showerror(\"Error\", \"Could not find a suitable text column for sentiment analysis.\")\n",
    "            return\n",
    "\n",
    "        total_rows = len(df)\n",
    "        results = []\n",
    "        sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0}\n",
    "        category_counts = {}\n",
    "\n",
    "        df[\"cleaned_tweet\"] = df[text_column].apply(clean_tweet)\n",
    "        df[\"category\"] = df[\"cleaned_tweet\"].apply(classify_tweet)\n",
    "\n",
    "        for category in df[\"category\"].unique():\n",
    "            category_counts[category] = len(df[df[\"category\"] == category])\n",
    "\n",
    "        for index, text in enumerate(df[text_column]):\n",
    "            if isinstance(text, str):\n",
    "                sentiment, polarity, error = analyze_sentiment(text)\n",
    "                if error:\n",
    "                    messagebox.showerror(\"Error\", error)\n",
    "                    return\n",
    "                results.append((index + 1, text, sentiment, polarity, df.loc[index, \"category\"]))\n",
    "                sentiment_counts[sentiment] += 1\n",
    "            else:\n",
    "                results.append((index + 1, text, \"N/A\", \"N/A\", df.loc[index, \"category\"]))\n",
    "\n",
    "            progress_var.set((index + 1) / total_rows * 100)\n",
    "            progress_label.config(text=f\"Processing: {int(progress_var.get())}%\")\n",
    "            root.update_idletasks()\n",
    "\n",
    "        for row in tree.get_children():\n",
    "            tree.delete(row)\n",
    "\n",
    "        for index, (serial, text, sentiment, polarity, category) in enumerate(results):\n",
    "            tag = \"oddrow\" if index % 2 == 0 else \"evenrow\"\n",
    "            tree.insert(\"\", tk.END, values=(serial, text, sentiment, polarity, category), tags=(tag,))\n",
    "\n",
    "        progress_label.config(text=\"Processing Completed.\")\n",
    "\n",
    "        global pie_chart_img, bar_chart_img, analyzed_df\n",
    "        pie_chart_img = create_pie_chart_image(sentiment_counts)\n",
    "        bar_chart_img = create_bar_chart_image(category_counts)\n",
    "\n",
    "        analyzed_df = pd.DataFrame(results, columns=[\"Serial\", \"Text\", \"Sentiment\", \"Polarity\", \"Category\"])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        messagebox.showerror(\"Error\", \"File not found.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "def create_bar_chart_image(category_counts):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=list(category_counts.keys()), y=list(category_counts.values()), palette=\"Set2\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Tweet Count\")\n",
    "    plt.title(\"Tweet Category Distribution\")\n",
    "    plt.xticks(rotation=30, fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    return img_tk\n",
    "\n",
    "def create_pie_chart_image(sentiment_counts):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    ax.axis('equal')\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    return img_tk  \n",
    "\n",
    "def analyze_dataset():\n",
    "    filepath = filedialog.askopenfilename(filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx;*.xls\"), (\"Text files\", \"*.txt\")])\n",
    "    if not filepath:\n",
    "        return\n",
    "    thread = threading.Thread(target=analyze_dataset_thread, args=(filepath,))\n",
    "    thread.start()\n",
    "\n",
    "def analyze_sentence():\n",
    "    text = text_entry.get(\"1.0\", tk.END).strip()\n",
    "    if not text:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a sentence.\")\n",
    "        return\n",
    "    sentiment, polarity, error = analyze_sentiment(text)\n",
    "    if error:\n",
    "        messagebox.showerror(\"Error\", error)\n",
    "        return\n",
    "    sentiment_label.config(text=f\"Sentiment: {sentiment}\")\n",
    "    polarity_label.config(text=f\"Polarity Score:{polarity:.4f}\")\n",
    "\n",
    "def show_charts():\n",
    "    if 'pie_chart_img' not in globals() or 'bar_chart_img' not in globals():\n",
    "        messagebox.showerror(\"Error\", \"Please analyze a dataset first.\")\n",
    "        return\n",
    "\n",
    "    charts_window = tk.Toplevel(root)\n",
    "    charts_window.title(\"Charts\")\n",
    "\n",
    "    # Create a frame to hold the charts side-by-side\n",
    "    charts_frame = tk.Frame(charts_window)\n",
    "    charts_frame.pack(pady=10)\n",
    "\n",
    "    pie_label = tk.Label(charts_frame, image=pie_chart_img)\n",
    "    pie_label.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "    bar_label = tk.Label(charts_frame, image=bar_chart_img)\n",
    "    bar_label.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "def download_table():\n",
    "    global analyzed_df\n",
    "    if 'analyzed_df' not in globals() or analyzed_df is None:\n",
    "        messagebox.showerror(\"Error\", \"Please analyze a dataset first.\")\n",
    "        return\n",
    "\n",
    "    filepath = filedialog.asksaveasfilename(defaultextension=\".csv\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "    if not filepath:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        analyzed_df.to_csv(filepath, index=False)\n",
    "        messagebox.showinfo(\"Success\", \"Table downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred during download: {e}\")\n",
    "\n",
    "# Create GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Sentiment Analysis and Category Classification\")\n",
    "root.state('zoomed')\n",
    "root.config(bg=\"#f0f8ff\")\n",
    "\n",
    "title_label = tk.Label(root, text=\"Sentiment Analysis and Category Classification\", font=(\"Helvetica\", 18, \"bold\"), bg=\"#f0f8ff\", fg=\"#4b8b3b\")\n",
    "title_label.pack(pady=10)\n",
    "\n",
    "text_entry = tk.Text(root, height=5, width=90, font=(\"Helvetica\", 12))\n",
    "text_entry.pack(pady=10)\n",
    "\n",
    "analyze_sentence_button = tk.Button(root, text=\"Analyze Sentence\", command=analyze_sentence, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "analyze_sentence_button.pack(pady=5)\n",
    "\n",
    "sentiment_label = tk.Label(root, text=\"Sentiment: \", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f0f8ff\")\n",
    "sentiment_label.pack()\n",
    "\n",
    "polarity_label = tk.Label(root, text=\"Polarity Score: \", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f0f8ff\")\n",
    "polarity_label.pack()\n",
    "\n",
    "# Create a frame to hold the buttons horizontally\n",
    "button_frame = tk.Frame(root, bg=\"#f0f8ff\")\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "analyze_dataset_button = tk.Button(button_frame, text=\"Analyze Dataset\", command=analyze_dataset, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "analyze_dataset_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "show_charts_button = tk.Button(button_frame, text=\"Show Charts\", command=show_charts, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "show_charts_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "download_table_button = tk.Button(button_frame, text=\"Download Table\", command=download_table, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "download_table_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "progress_frame = tk.Frame(root, bg=\"#f0f8ff\")\n",
    "progress_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "progress_var = tk.DoubleVar()\n",
    "progress_bar = ttk.Progressbar(progress_frame, variable=progress_var, maximum=100)\n",
    "progress_bar.pack(pady=3, fill=tk.X, padx=20)\n",
    "\n",
    "progress_label = tk.Label(progress_frame, text=\"Processing: 0%\", bg=\"#f0f8ff\", font=(\"Helvetica\", 12))\n",
    "progress_label.pack(pady=5)\n",
    "\n",
    "style = ttk.Style()\n",
    "style.configure(\"Treeview\", rowheight=25, font=(\"Helvetica\", 12))\n",
    "style.configure(\"Treeview.Heading\", font=(\"Helvetica\", 12, \"bold\"))\n",
    "style.layout(\"Treeview\", [('Treeview.treearea', {'sticky': 'nswe'})])\n",
    "style.map(\"Treeview\", background=[(\"selected\", \"#4b8b3b\")])\n",
    "\n",
    "tree = ttk.Treeview(progress_frame, columns=(\"Serial\", \"Text\", \"Sentiment\", \"Polarity\", \"Category\"), show=\"headings\", height=10, style=\"Treeview\")\n",
    "tree.heading(\"Serial\", text=\"Serial\")\n",
    "tree.heading(\"Text\", text=\"Text\")\n",
    "tree.heading(\"Sentiment\", text=\"Sentiment\")\n",
    "tree.heading(\"Polarity\", text=\"Polarity\")\n",
    "tree.heading(\"Category\", text=\"Category\")\n",
    "\n",
    "tree.column(\"Serial\", width=50, anchor=\"center\")\n",
    "tree.column(\"Text\", width=200, anchor=\"center\")\n",
    "tree.column(\"Sentiment\", width=100, anchor=\"center\")\n",
    "tree.column(\"Polarity\", width=80, anchor=\"center\")\n",
    "tree.column(\"Category\", width=120, anchor=\"center\")\n",
    "\n",
    "tree[\"show\"] = \"headings\"\n",
    "tree.tag_configure(\"oddrow\", background=\"#f9f9f9\")\n",
    "tree.tag_configure(\"evenrow\", background=\"#e6e6e6\")\n",
    "tree.pack(fill=tk.BOTH, expand=True, side=tk.LEFT)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ac852-f783-40f8-9024-857d2b0e0407",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d81e912-166b-49eb-b8c1-48b6c7ecb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageTk\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import Image, ImageTk  # Import PIL for image handling\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load the saved SVM model, vectorizer, and scaler\n",
    "svm_model = joblib.load(\"sentiment_svm_model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment of a single text string.\"\"\"\n",
    "    sentence_polarity = sia.polarity_scores(text)[\"compound\"]\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    polarity_array = np.array([[sentence_polarity]])\n",
    "    scaled_polarity = scaler.transform(polarity_array)\n",
    "    scaled_polarity_sparse = csr_matrix(scaled_polarity)\n",
    "    combined_features = hstack([text_tfidf, scaled_polarity_sparse])\n",
    "\n",
    "    if combined_features.shape[1] != svm_model.n_features_in_:\n",
    "        return None, None, f\"Feature mismatch: Model expects {svm_model.n_features_in_} features, but got {combined_features.shape[1]}.\"\n",
    "\n",
    "    prediction = svm_model.predict(combined_features)[0]\n",
    "\n",
    "    if sentence_polarity > 0.1:\n",
    "        sentiment = \"Positive\"\n",
    "    elif -0.1 <= sentence_polarity <= 0.1:\n",
    "        sentiment = \"Neutral\"\n",
    "    else:\n",
    "        sentiment = \"Negative\"\n",
    "\n",
    "    return sentiment, sentence_polarity, None  # No error\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "categories = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"AI\", \"data\", \"computer\", \"cloud\", \"robotics\", \"internet\"],\n",
    "    \"Entertainment\": [\"movie\", \"music\", \"show\", \"film\", \"concert\", \"actor\", \"director\"],\n",
    "    \"Politics\": [\"election\", \"government\", \"president\", \"policy\", \"vote\", \"law\", \"senate\"],\n",
    "    \"Sports\": [\"football\", \"basketball\", \"cricket\", \"match\", \"goal\", \"tennis\", \"athlete\"],\n",
    "    \"General Fun\": [\"fun\", \"joke\", \"meme\", \"party\", \"game\", \"laugh\", \"entertainment\"],\n",
    "    \"News\": [\"breaking\", \"news\", \"report\", \"update\", \"headline\", \"journalist\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"doctor\", \"hospital\", \"patient\", \"disease\", \"treatment\", \"vaccine\", \"covid\", \"pandemic\", \"symptom\", \"diagnosis\", \"pharmacy\", \"medicine\", \"virus\", \"wellness\"],\n",
    "    \"Education\": [\"school\", \"college\", \"university\", \"student\", \"teacher\", \"learn\", \"study\", \"education\"],\n",
    "    \"Finance\": [\"stock\", \"market\", \"money\", \"bank\", \"finance\", \"investment\", \"economy\"],\n",
    "    \"Travel\": [\"travel\", \"trip\", \"vacation\", \"tour\", \"flight\", \"hotel\", \"destination\"],\n",
    "    \"Food\": [\"food\", \"recipe\", \"restaurant\", \"cuisine\", \"eat\", \"cook\", \"dish\"],\n",
    "    \"Fashion\": [\"fashion\", \"style\", \"clothing\", \"design\", \"trend\", \"apparel\"],\n",
    "    \"Environment\": [\"climate\", \"environment\", \"pollution\", \"sustainability\", \"eco\", \"green\"],\n",
    "    \"Business\": [\"business\", \"company\", \"startup\", \"entrepreneur\", \"market\", \"sales\", \"product\", \"innovation\"],\n",
    "    \"Art\": [\"art\", \"painting\", \"sculpture\", \"artist\", \"exhibition\", \"gallery\", \"creative\"],\n",
    "    \"Books\": [\"book\", \"reading\", \"author\", \"literature\", \"novel\", \"poetry\", \"library\"],\n",
    "    \"Gaming\": [\"game\", \"gaming\", \"videogame\", \"console\", \"player\", \"esports\", \"virtual\"]\n",
    "}\n",
    "\n",
    "def classify_tweet(text):\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in text for word in keywords):\n",
    "            return category\n",
    "    return \"Other\"\n",
    "\n",
    "def analyze_dataset_thread(filepath):\n",
    "    \"\"\"Analyzes sentiment and classifies tweets of a dataset in a separate thread and updates the UI in the same window.\"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath)\n",
    "        elif filepath.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(filepath)\n",
    "        elif filepath.endswith('.txt'):\n",
    "            df = pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "        text_column = None\n",
    "        for col in df.columns:\n",
    "            if \"text\" in col.lower() or \"review\" in col.lower() or \"comment\" in col.lower() or \"content\" in col.lower() or \"message\" in col.lower() or \"description\" in col.lower():\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if text_column is None:\n",
    "            messagebox.showerror(\"Error\", \"Could not find a suitable text column for sentiment analysis.\")\n",
    "            return\n",
    "\n",
    "        total_rows = len(df)\n",
    "        results = []\n",
    "        sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0}\n",
    "        category_counts = {}\n",
    "\n",
    "        df[\"cleaned_tweet\"] = df[text_column].apply(clean_tweet)\n",
    "        df[\"category\"] = df[\"cleaned_tweet\"].apply(classify_tweet)\n",
    "\n",
    "        for category in df[\"category\"].unique():\n",
    "            category_counts[category] = len(df[df[\"category\"] == category])\n",
    "\n",
    "        for index, text in enumerate(df[text_column]):\n",
    "            if isinstance(text, str):\n",
    "                sentiment, polarity, error = analyze_sentiment(text)\n",
    "                if error:\n",
    "                    messagebox.showerror(\"Error\", error)\n",
    "                    return\n",
    "                results.append((index + 1, text, sentiment, polarity, df.loc[index, \"category\"]))\n",
    "                sentiment_counts[sentiment] += 1\n",
    "            else:\n",
    "                results.append((index + 1, text, \"N/A\", \"N/A\", df.loc[index, \"category\"]))\n",
    "\n",
    "            progress_var.set((index + 1) / total_rows * 100)\n",
    "            progress_label.config(text=f\"Processing: {int(progress_var.get())}%\")\n",
    "            root.update_idletasks()\n",
    "\n",
    "        for row in tree.get_children():\n",
    "            tree.delete(row)\n",
    "\n",
    "        for index, (serial, text, sentiment, polarity, category) in enumerate(results):\n",
    "            tag = \"oddrow\" if index % 2 == 0 else \"evenrow\"\n",
    "            tree.insert(\"\", tk.END, values=(serial, text, sentiment, polarity, category), tags=(tag,))\n",
    "\n",
    "        progress_label.config(text=\"Processing Completed.\")\n",
    "\n",
    "        global pie_chart_img, bar_chart_img, analyzed_df\n",
    "        pie_chart_img = create_pie_chart_image(sentiment_counts)\n",
    "        bar_chart_img = create_bar_chart_image(category_counts)\n",
    "\n",
    "        analyzed_df = pd.DataFrame(results, columns=[\"Serial\", \"Text\", \"Sentiment\", \"Polarity\", \"Category\"])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        messagebox.showerror(\"Error\", \"File not found.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "def create_bar_chart_image(category_counts):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=list(category_counts.keys()), y=list(category_counts.values()), palette=\"Set2\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Tweet Count\")\n",
    "    plt.title(\"Tweet Category Distribution\")\n",
    "    plt.xticks(rotation=30, fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    return img_tk\n",
    "\n",
    "def create_pie_chart_image(sentiment_counts):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.pie(sentiment_counts.values(), labels=sentiment_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    ax.axis('equal')\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    img_tk = ImageTk.PhotoImage(img)\n",
    "    return img_tk\n",
    "\n",
    "def analyze_dataset():\n",
    "    filepath = filedialog.askopenfilename(filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx;*.xls\"), (\"Text files\", \"*.txt\")])\n",
    "    if not filepath:\n",
    "        return\n",
    "    thread = threading.Thread(target=analyze_dataset_thread, args=(filepath,))\n",
    "    thread.start()\n",
    "\n",
    "def analyze_sentence():\n",
    "    text = text_entry.get(\"1.0\", tk.END).strip()\n",
    "    if not text:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a sentence.\")\n",
    "        return\n",
    "    sentiment, polarity, error = analyze_sentiment(text)\n",
    "    if error:\n",
    "        messagebox.showerror(\"Error\", error)\n",
    "        return\n",
    "    sentiment_label.config(text=f\"Sentiment: {sentiment}\")\n",
    "    polarity_label.config(text=f\"Polarity Score:{polarity:.4f}\")\n",
    "\n",
    "def show_charts():\n",
    "    if 'pie_chart_img' not in globals() or 'bar_chart_img' not in globals():\n",
    "        messagebox.showerror(\"Error\", \"Please analyze a dataset first.\")\n",
    "        return\n",
    "\n",
    "    charts_window = tk.Toplevel(root)\n",
    "    charts_window.title(\"Charts\")\n",
    "\n",
    "    # Create a frame to hold the charts side-by-side\n",
    "    charts_frame = tk.Frame(charts_window)\n",
    "    charts_frame.pack(pady=10)\n",
    "\n",
    "    pie_label = tk.Label(charts_frame, image=pie_chart_img)\n",
    "    pie_label.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "    bar_label = tk.Label(charts_frame, image=bar_chart_img)\n",
    "    bar_label.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "def download_table():\n",
    "    global analyzed_df\n",
    "    if 'analyzed_df' not in globals() or analyzed_df is None:\n",
    "        messagebox.showerror(\"Error\", \"Please analyze a dataset first.\")\n",
    "        return\n",
    "\n",
    "    filepath = filedialog.asksaveasfilename(defaultextension=\".csv\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "    if not filepath:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        analyzed_df.to_csv(filepath, index=False)\n",
    "        messagebox.showinfo(\"Success\", \"Table downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred during download: {e}\")\n",
    "\n",
    "# Create GUI\n",
    "def main_gui():\n",
    "    global root, text_entry, sentiment_label, polarity_label, progress_var, progress_bar, progress_label, tree\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Sentiment Analysis and Category Classification\")\n",
    "    root.state('zoomed')\n",
    "    root.config(bg=\"#f0f8ff\")\n",
    "\n",
    "    title_label = tk.Label(root, text=\"Sentiment Analysis and Category Classification\", font=(\"Helvetica\", 18, \"bold\"), bg=\"#f0f8ff\", fg=\"#4b8b3b\")\n",
    "    title_label.pack(pady=10)\n",
    "\n",
    "    text_entry = tk.Text(root, height=5, width=90, font=(\"Helvetica\", 12))\n",
    "    text_entry.pack(pady=10)\n",
    "\n",
    "    analyze_sentence_button = tk.Button(root, text=\"Analyze Sentence\", command=analyze_sentence, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "    analyze_sentence_button.pack(pady=5)\n",
    "\n",
    "    sentiment_label = tk.Label(root, text=\"Sentiment: \", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f0f8ff\")\n",
    "    sentiment_label.pack()\n",
    "\n",
    "    polarity_label = tk.Label(root, text=\"Polarity Score: \", font=(\"Helvetica\", 12, \"bold\"), bg=\"#f0f8ff\")\n",
    "    polarity_label.pack()\n",
    "\n",
    "    # Create a frame to hold the buttons horizontally\n",
    "    button_frame = tk.Frame(root, bg=\"#f0f8ff\")\n",
    "    button_frame.pack(pady=10)\n",
    "\n",
    "    analyze_dataset_button = tk.Button(button_frame, text=\"Analyze Dataset\", command=analyze_dataset, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "    analyze_dataset_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "    show_charts_button = tk.Button(button_frame, text=\"Show Charts\", command=show_charts, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "    show_charts_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "    download_table_button = tk.Button(button_frame, text=\"Download Table\", command=download_table, font=(\"Helvetica\", 12), bg=\"#4b8b3b\", fg=\"white\", relief=\"raised\", width=20)\n",
    "    download_table_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "    progress_frame = tk.Frame(root, bg=\"#f0f8ff\")\n",
    "    progress_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    progress_var = tk.DoubleVar()\n",
    "    progress_bar = ttk.Progressbar(progress_frame, variable=progress_var, maximum=100)\n",
    "    progress_bar.pack(pady=3, fill=tk.X, padx=20)\n",
    "\n",
    "    progress_label = tk.Label(progress_frame, text=\"Processing: 0%\", bg=\"#f0f8ff\", font=(\"Helvetica\", 12))\n",
    "    progress_label.pack(pady=5)\n",
    "\n",
    "    style = ttk.Style()\n",
    "    style.configure(\"Treeview\", rowheight=25, font=(\"Helvetica\", 12))\n",
    "    style.configure(\"Treeview.Heading\", font=(\"Helvetica\", 12, \"bold\"))\n",
    "    style.layout(\"Treeview\", [('Treeview.treearea', {'sticky': 'nswe'})])\n",
    "    style.map(\"Treeview\", background=[(\"selected\", \"#4b8b3b\")])\n",
    "\n",
    "    tree = ttk.Treeview(progress_frame, columns=(\"Serial\", \"Text\", \"Sentiment\", \"Polarity\", \"Category\"), show=\"headings\", height=10, style=\"Treeview\")\n",
    "    tree.heading(\"Serial\", text=\"Serial\")\n",
    "    tree.heading(\"Text\", text=\"Text\")\n",
    "    tree.heading(\"Sentiment\", text=\"Sentiment\")\n",
    "    tree.heading(\"Polarity\", text=\"Polarity\")\n",
    "    tree.heading(\"Category\", text=\"Category\")\n",
    "\n",
    "    tree.column(\"Serial\", width=50, anchor=\"center\")\n",
    "    tree.column(\"Text\", width=200, anchor=\"center\")\n",
    "    tree.column(\"Sentiment\", width=100, anchor=\"center\")\n",
    "    tree.column(\"Polarity\", width=80, anchor=\"center\")\n",
    "    tree.column(\"Category\", width=120, anchor=\"center\")\n",
    "\n",
    "    tree[\"show\"] = \"headings\"\n",
    "    tree.tag_configure(\"oddrow\", background=\"#f9f9f9\")\n",
    "    tree.tag_configure(\"evenrow\", background=\"#e6e6e6\")\n",
    "    tree.pack(fill=tk.BOTH, expand=True, side=tk.LEFT)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def login():\n",
    "    login_window = tk.Tk()\n",
    "    login_window.title(\"Login\")\n",
    "    login_window.state('zoomed')\n",
    "    login_window.configure(bg=\"#e0f2fe\")  # Light blue/gray background\n",
    "\n",
    "    # Title Frame with Green Background Strip\n",
    "    title_frame = tk.Frame(login_window, bg=\"#4b8b3b\")  # Green background strip\n",
    "    title_frame.pack(fill=tk.X, pady=(40, 0))  # Fill horizontally, top padding\n",
    "\n",
    "    title_label = tk.Label(title_frame, text=\"SIGN IN\", font=(\"Roboto\", 24, \"bold\"), bg=\"#4b8b3b\", fg=\"white\")  # White text on green\n",
    "    title_label.pack(pady=10)  # Padding inside the frame\n",
    "\n",
    "    # Username - Bold Label\n",
    "    username_label = tk.Label(login_window, text=\"Username\", font=(\"Roboto\", 14, \"bold\"), bg=\"#e0f2fe\", fg=\"#666\")\n",
    "    username_label.pack(pady=(20, 2))  # Increased top padding\n",
    "    username_entry = tk.Entry(login_window, font=(\"Roboto\", 12), width=30, borderwidth=2, relief=tk.FLAT)\n",
    "    username_entry.pack(pady=(2, 10))\n",
    "\n",
    "    # Password - Bold Label\n",
    "    password_label = tk.Label(login_window, text=\"Password\", font=(\"Roboto\", 14, \"bold\"), bg=\"#e0f2fe\", fg=\"#666\")\n",
    "    password_label.pack(pady=(5, 2))\n",
    "    password_entry = tk.Entry(login_window, show=\"*\", font=(\"Roboto\", 12), width=30, borderwidth=2, relief=tk.FLAT)\n",
    "    password_entry.pack(pady=(2, 10))\n",
    "\n",
    "    def check_login(event=None):  # Add event parameter\n",
    "        username = username_entry.get()\n",
    "        password = password_entry.get()\n",
    "        # Replace with your actual authentication logic\n",
    "        if username == \"user\" and password == \"pass\":\n",
    "            login_window.destroy()\n",
    "            main_gui()  # Call main_gui() after successful login\n",
    "        else:\n",
    "            messagebox.showerror(\"Login Failed\", \"Incorrect username or password.\")\n",
    "\n",
    "    # Login Button\n",
    "    login_button = tk.Button(login_window, text=\"Sign In\", command=check_login, font=(\"Roboto\", 12, \"bold\"), bg=\"#4b8b3b\", fg=\"white\", padx=20, pady=10, relief=tk.FLAT, borderwidth=0)\n",
    "    login_button.pack(pady=(20, 20))\n",
    "\n",
    "    # Bind Enter key to check_login function\n",
    "    login_window.bind('<Return>', check_login)\n",
    "\n",
    "    login_window.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca54b4-953d-4346-989f-be0b8b4b2018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcien",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
